{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1k8wST5WcpVxdJeQ2LEwRP5yyPK8NiZBg","timestamp":1691618838444}],"authorship_tag":"ABX9TyP+TF0hid8Tij9fhwHZJiKx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ywd60paBfNGT","executionInfo":{"status":"ok","timestamp":1691611055706,"user_tz":-330,"elapsed":35360,"user":{"displayName":"soumalya seal","userId":"15338709408862813228"}},"outputId":"0f93b84a-489a-4c3e-bc4e-c2acedfb8e5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Collecting datasets\n","  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting evaluate\n","  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n","Collecting responses<0.19 (from evaluate)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.6)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n","Building wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=1d2172398b003b9a8d09e243ff7e3d9a377107f373a55669bbe24bf2bdb0c4da\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge_score\n","Installing collected packages: xxhash, dill, rouge_score, responses, multiprocess, datasets, evaluate\n","Successfully installed datasets-2.14.4 dill-0.3.7 evaluate-0.4.0 multiprocess-0.70.15 responses-0.18.0 rouge_score-0.1.2 xxhash-3.3.0\n"]}],"source":["!pip install transformers #Using HuggingFace for getting the Google Flan-T5-small model\n","!pip install sentencepiece #T5 tokenizer uses SentencePiece tokenizer\n","!pip install transformers datasets evaluate rouge_score"]},{"cell_type":"code","source":["import torch"],"metadata":{"id":"CZ9QQGwIQR1_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoConfig\n","\n","model_name = \"google/flan-t5-small\"\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","config = AutoConfig.from_pretrained(model_name)"],"metadata":{"id":"XMFIhybQf_V5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nsw0uFTSN2iT","executionInfo":{"status":"ok","timestamp":1691616771300,"user_tz":-330,"elapsed":4,"user":{"displayName":"soumalya seal","userId":"15338709408862813228"}},"outputId":"a380c171-6f31-4265-99c5-4853cf08b7eb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["T5Config {\n","  \"_name_or_path\": \"google/flan-t5-small\",\n","  \"architectures\": [\n","    \"T5ForConditionalGeneration\"\n","  ],\n","  \"d_ff\": 1024,\n","  \"d_kv\": 64,\n","  \"d_model\": 512,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"gelu_new\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"gated-gelu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_decoder_layers\": 8,\n","  \"num_heads\": 6,\n","  \"num_layers\": 8,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 200,\n","      \"min_length\": 30,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4,\n","      \"prefix\": \"summarize: \"\n","    },\n","    \"translation_en_to_de\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to German: \"\n","    },\n","    \"translation_en_to_fr\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to French: \"\n","    },\n","    \"translation_en_to_ro\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to Romanian: \"\n","    }\n","  },\n","  \"tie_word_embeddings\": false,\n","  \"transformers_version\": \"4.31.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32128\n","}"]},"metadata":{},"execution_count":53}]},{"cell_type":"markdown","source":["# Testing Text Summarization"],"metadata":{"id":"qI4TZbnwuUOW"}},{"cell_type":"code","source":["input_text = '''\n","Text Summarization is a natural language processing (NLP) task that involves condensing a lengthy text document into a shorter, more compact version while still retaining the most important information and meaning. The goal is to produce a summary that accurately represents the content of the original text in a concise form. There are different approaches to text summarization, including extractive methods that identify and extract important sentences or phrases from the text, and abstractive methods that generate new text based on the content of the original text.\n","'''"],"metadata":{"id":"8XDTexKuyv6n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = tokenizer.encode(\"summarize: \" + input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n","summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2, num_beams=4, early_stopping=True)\n","\n","summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","print(\"Generated Summary:\", summary)"],"metadata":{"id":"zLKRqzLujEpz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691611141982,"user_tz":-330,"elapsed":6181,"user":{"displayName":"soumalya seal","userId":"15338709408862813228"}},"outputId":"87bce38b-2d32-44ed-9e12-0d3d7af508da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated Summary: Text Summarization is a natural language processing (NLP) task that involves condensing a lengthy text document into a shorter, more compact version while still retaining the most important information and meaning.\n"]}]},{"cell_type":"code","source":["print(\"Input length: {}, Summary length:{}\".format(len(input_text.split(' ')),len(summary.split())))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9XhG_foCxRU0","executionInfo":{"status":"ok","timestamp":1691611141982,"user_tz":-330,"elapsed":12,"user":{"displayName":"soumalya seal","userId":"15338709408862813228"}},"outputId":"abb97982-9c75-4504-a866-25aaa404481e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input length: 87, Summary length:31\n"]}]},{"cell_type":"markdown","source":["# Testing Question Answering Task"],"metadata":{"id":"VDKxJPMow_qK"}},{"cell_type":"code","source":["context = \"The capital of France is Paris. France is known for its rich history and cultural heritage.\"\n","question = \"What is the capital of France?\""],"metadata":{"id":"ThImPaBSubxM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_text = f\"question: {question} context: {context}\"\n","input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n","\n","answer_ids = model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)\n","answer = tokenizer.decode(answer_ids[0], skip_special_tokens=True)\n","print(\"Answer:\", answer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6CIwm0xY0iuv","executionInfo":{"status":"ok","timestamp":1691611142687,"user_tz":-330,"elapsed":713,"user":{"displayName":"soumalya seal","userId":"15338709408862813228"}},"outputId":"c5df3533-7f9e-4f91-ea1d-97f79b028347"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Answer: Paris\n"]}]},{"cell_type":"markdown","source":["# Translation Task"],"metadata":{"id":"tAlcJuCdOxKY"}},{"cell_type":"code","source":["english_text = \"This is an example English sentence that you want to translate.\""],"metadata":{"id":"yHe1mbco0o2v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_text = \"translate English to French: \" + english_text\n","input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n","\n","translated_ids = model.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)\n","translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n","print(\"Translated Text:\", translated_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YA-6HPf_0wSS","executionInfo":{"status":"ok","timestamp":1691611143099,"user_tz":-330,"elapsed":413,"user":{"displayName":"soumalya seal","userId":"15338709408862813228"}},"outputId":"fb2f50af-bb6b-4e22-b7d6-e0df38f02f03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Translated Text: Cela est une exemple anglaise que vous voulez.\n"]}]},{"cell_type":"markdown","source":["# Model Parameters"],"metadata":{"id":"92npRb3l1Ojt"}},{"cell_type":"code","source":["# Get the layer names and parameters\n","for name, param in model.named_parameters():\n","    print(f\"Layer name: {name}, Parameter shape: {param.shape}\")"],"metadata":{"id":"ZU88fncJ01ny","executionInfo":{"status":"ok","timestamp":1691611143099,"user_tz":-330,"elapsed":2,"user":{"displayName":"soumalya seal","userId":"15338709408862813228"}},"outputId":"8e91be80-fdca-41e7-f706-c9ac7f02880c","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer name: shared.weight, Parameter shape: torch.Size([32128, 512])\n","Layer name: encoder.block.0.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.0.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.0.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.0.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Parameter shape: torch.Size([32, 6])\n","Layer name: encoder.block.0.layer.0.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: encoder.block.0.layer.1.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: encoder.block.0.layer.1.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: encoder.block.0.layer.1.DenseReluDense.wo.weight, Parameter shape: torch.Size([512, 1024])\n","Layer name: encoder.block.0.layer.1.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: encoder.block.1.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.1.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.1.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.1.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: encoder.block.1.layer.0.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: encoder.block.1.layer.1.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: encoder.block.1.layer.1.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: encoder.block.1.layer.1.DenseReluDense.wo.weight, Parameter shape: torch.Size([512, 1024])\n","Layer name: encoder.block.1.layer.1.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: encoder.block.2.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.2.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.2.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.2.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: encoder.block.2.layer.0.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: encoder.block.2.layer.1.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: encoder.block.2.layer.1.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: encoder.block.2.layer.1.DenseReluDense.wo.weight, Parameter shape: torch.Size([512, 1024])\n","Layer name: encoder.block.2.layer.1.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: encoder.block.3.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.3.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.3.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.3.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: encoder.block.3.layer.0.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: encoder.block.3.layer.1.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: encoder.block.3.layer.1.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: encoder.block.3.layer.1.DenseReluDense.wo.weight, Parameter shape: torch.Size([512, 1024])\n","Layer name: encoder.block.3.layer.1.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: encoder.block.4.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.4.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.4.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.4.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: encoder.block.4.layer.0.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: encoder.block.4.layer.1.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: encoder.block.4.layer.1.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: encoder.block.4.layer.1.DenseReluDense.wo.weight, Parameter shape: torch.Size([512, 1024])\n","Layer name: encoder.block.4.layer.1.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: encoder.block.5.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.5.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.5.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.5.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: encoder.block.5.layer.0.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: encoder.block.5.layer.1.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: encoder.block.5.layer.1.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: encoder.block.5.layer.1.DenseReluDense.wo.weight, Parameter shape: torch.Size([512, 1024])\n","Layer name: encoder.block.5.layer.1.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: encoder.block.6.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.6.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.6.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.6.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: encoder.block.6.layer.0.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: encoder.block.6.layer.1.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: encoder.block.6.layer.1.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: encoder.block.6.layer.1.DenseReluDense.wo.weight, Parameter shape: torch.Size([512, 1024])\n","Layer name: encoder.block.6.layer.1.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: encoder.block.7.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.7.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.7.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: encoder.block.7.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: encoder.block.7.layer.0.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: encoder.block.7.layer.1.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: encoder.block.7.layer.1.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: encoder.block.7.layer.1.DenseReluDense.wo.weight, Parameter shape: torch.Size([512, 1024])\n","Layer name: encoder.block.7.layer.1.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: encoder.final_layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.0.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.0.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.0.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.0.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Parameter shape: torch.Size([32, 6])\n","Layer name: decoder.block.0.layer.0.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.0.layer.1.EncDecAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.0.layer.1.EncDecAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.0.layer.1.EncDecAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.0.layer.1.EncDecAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: decoder.block.0.layer.1.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.0.layer.2.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: decoder.block.0.layer.2.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: decoder.block.0.layer.2.DenseReluDense.wo.weight, Parameter shape: torch.Size([512, 1024])\n","Layer name: decoder.block.0.layer.2.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.1.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.1.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.1.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.1.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: decoder.block.1.layer.0.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.1.layer.1.EncDecAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.1.layer.1.EncDecAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.1.layer.1.EncDecAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.1.layer.1.EncDecAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: decoder.block.1.layer.1.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.1.layer.2.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: decoder.block.1.layer.2.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: decoder.block.1.layer.2.DenseReluDense.wo.weight, Parameter shape: torch.Size([512, 1024])\n","Layer name: decoder.block.1.layer.2.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.2.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.2.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.2.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.2.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: decoder.block.2.layer.0.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.2.layer.1.EncDecAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.2.layer.1.EncDecAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.2.layer.1.EncDecAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.2.layer.1.EncDecAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: decoder.block.2.layer.1.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.2.layer.2.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: decoder.block.2.layer.2.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: decoder.block.2.layer.2.DenseReluDense.wo.weight, Parameter shape: torch.Size([512, 1024])\n","Layer name: decoder.block.2.layer.2.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.3.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.3.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.3.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.3.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: decoder.block.3.layer.0.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.3.layer.1.EncDecAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.3.layer.1.EncDecAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.3.layer.1.EncDecAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.3.layer.1.EncDecAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: decoder.block.3.layer.1.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.3.layer.2.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: decoder.block.3.layer.2.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: decoder.block.3.layer.2.DenseReluDense.wo.weight, Parameter shape: torch.Size([512, 1024])\n","Layer name: decoder.block.3.layer.2.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.4.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.4.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.4.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.4.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: decoder.block.4.layer.0.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.4.layer.1.EncDecAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.4.layer.1.EncDecAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.4.layer.1.EncDecAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.4.layer.1.EncDecAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: decoder.block.4.layer.1.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.4.layer.2.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: decoder.block.4.layer.2.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: decoder.block.4.layer.2.DenseReluDense.wo.weight, Parameter shape: torch.Size([512, 1024])\n","Layer name: decoder.block.4.layer.2.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.5.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.5.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.5.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.5.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: decoder.block.5.layer.0.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.5.layer.1.EncDecAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.5.layer.1.EncDecAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.5.layer.1.EncDecAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.5.layer.1.EncDecAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: decoder.block.5.layer.1.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.5.layer.2.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: decoder.block.5.layer.2.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: decoder.block.5.layer.2.DenseReluDense.wo.weight, Parameter shape: torch.Size([512, 1024])\n","Layer name: decoder.block.5.layer.2.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.6.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.6.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.6.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.6.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: decoder.block.6.layer.0.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.6.layer.1.EncDecAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.6.layer.1.EncDecAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.6.layer.1.EncDecAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.6.layer.1.EncDecAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: decoder.block.6.layer.1.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.6.layer.2.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: decoder.block.6.layer.2.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: decoder.block.6.layer.2.DenseReluDense.wo.weight, Parameter shape: torch.Size([512, 1024])\n","Layer name: decoder.block.6.layer.2.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.7.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.7.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.7.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.7.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: decoder.block.7.layer.0.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.7.layer.1.EncDecAttention.q.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.7.layer.1.EncDecAttention.k.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.7.layer.1.EncDecAttention.v.weight, Parameter shape: torch.Size([384, 512])\n","Layer name: decoder.block.7.layer.1.EncDecAttention.o.weight, Parameter shape: torch.Size([512, 384])\n","Layer name: decoder.block.7.layer.1.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.block.7.layer.2.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: decoder.block.7.layer.2.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 512])\n","Layer name: decoder.block.7.layer.2.DenseReluDense.wo.weight, Parameter shape: torch.Size([512, 1024])\n","Layer name: decoder.block.7.layer.2.layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: decoder.final_layer_norm.weight, Parameter shape: torch.Size([512])\n","Layer name: lm_head.weight, Parameter shape: torch.Size([32128, 512])\n"]}]},{"cell_type":"code","source":["# Calculate total number of parameters\n","total_parameters = sum(param.numel() for param in model.parameters())\n","print(\"\\nTotal Number of Parameters:\", total_parameters)"],"metadata":{"id":"6x8U51sP1wC0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691611161351,"user_tz":-330,"elapsed":502,"user":{"displayName":"soumalya seal","userId":"15338709408862813228"}},"outputId":"7357dddc-1354-4c79-8e20-95e5ddf07af1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Total Number of Parameters: 76961152\n"]}]},{"cell_type":"code","source":["# Set the tensor in the final layer to all zeros\n","weight_backup = model.decoder.final_layer_norm.weight\n","model.decoder.final_layer_norm.weight.data.fill_(0.0)\n","\n","# Verify the change\n","print(\"Updated final layer norm weights:\", model.decoder.final_layer_norm.weight)"],"metadata":{"id":"uH8Wk6arN1eE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691611187455,"user_tz":-330,"elapsed":370,"user":{"displayName":"soumalya seal","userId":"15338709408862813228"}},"outputId":"b700136b-55fe-4019-96fc-6464412a3ca0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Updated final layer norm weights: Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n"]}]},{"cell_type":"markdown","source":["# Model Modification"],"metadata":{"id":"65NWARNxPRHr"}},{"cell_type":"code","source":["# Reducing the dimension to 128\n","new_dim = 256\n","modified_config = config\n","\n","modified_config.d_model = new_dim  # Update the hidden dimension\n","modified_config.num_heads = new_dim // 32  # Adjust the number of attention heads\n","\n","\n","# Load the model with the modified configuration\n","modified_model = T5ForConditionalGeneration(config=modified_config)\n","\n","# Verify changes\n","print(\"Updated model configuration:\", modified_model.config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gB6QO2NLPNJ6","executionInfo":{"status":"ok","timestamp":1691617776371,"user_tz":-330,"elapsed":2885,"user":{"displayName":"soumalya seal","userId":"15338709408862813228"}},"outputId":"51f370be-5160-4fb8-98c4-d8a47bfaa88f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Updated model configuration: T5Config {\n","  \"_name_or_path\": \"google/flan-t5-small\",\n","  \"architectures\": [\n","    \"T5ForConditionalGeneration\"\n","  ],\n","  \"d_ff\": 1024,\n","  \"d_kv\": 64,\n","  \"d_model\": 256,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"gelu_new\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"gated-gelu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_decoder_layers\": 8,\n","  \"num_heads\": 8,\n","  \"num_layers\": 8,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 200,\n","      \"min_length\": 30,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4,\n","      \"prefix\": \"summarize: \"\n","    },\n","    \"translation_en_to_de\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to German: \"\n","    },\n","    \"translation_en_to_fr\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to French: \"\n","    },\n","    \"translation_en_to_ro\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to Romanian: \"\n","    }\n","  },\n","  \"tie_word_embeddings\": false,\n","  \"transformers_version\": \"4.31.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32128\n","}\n","\n"]}]},{"cell_type":"code","source":["# Get the layer names and parameters\n","for name, param in modified_model.named_parameters():\n","    print(f\"Layer name: {name}, Parameter shape: {param.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Max3ou7LRz8K","executionInfo":{"status":"ok","timestamp":1691617815202,"user_tz":-330,"elapsed":555,"user":{"displayName":"soumalya seal","userId":"15338709408862813228"}},"outputId":"874fe7f6-2a0e-44be-cb04-0b4779f2ab19"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer name: shared.weight, Parameter shape: torch.Size([32128, 256])\n","Layer name: encoder.block.0.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.0.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.0.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.0.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Parameter shape: torch.Size([32, 8])\n","Layer name: encoder.block.0.layer.0.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: encoder.block.0.layer.1.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: encoder.block.0.layer.1.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: encoder.block.0.layer.1.DenseReluDense.wo.weight, Parameter shape: torch.Size([256, 1024])\n","Layer name: encoder.block.0.layer.1.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: encoder.block.1.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.1.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.1.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.1.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: encoder.block.1.layer.0.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: encoder.block.1.layer.1.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: encoder.block.1.layer.1.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: encoder.block.1.layer.1.DenseReluDense.wo.weight, Parameter shape: torch.Size([256, 1024])\n","Layer name: encoder.block.1.layer.1.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: encoder.block.2.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.2.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.2.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.2.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: encoder.block.2.layer.0.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: encoder.block.2.layer.1.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: encoder.block.2.layer.1.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: encoder.block.2.layer.1.DenseReluDense.wo.weight, Parameter shape: torch.Size([256, 1024])\n","Layer name: encoder.block.2.layer.1.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: encoder.block.3.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.3.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.3.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.3.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: encoder.block.3.layer.0.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: encoder.block.3.layer.1.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: encoder.block.3.layer.1.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: encoder.block.3.layer.1.DenseReluDense.wo.weight, Parameter shape: torch.Size([256, 1024])\n","Layer name: encoder.block.3.layer.1.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: encoder.block.4.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.4.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.4.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.4.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: encoder.block.4.layer.0.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: encoder.block.4.layer.1.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: encoder.block.4.layer.1.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: encoder.block.4.layer.1.DenseReluDense.wo.weight, Parameter shape: torch.Size([256, 1024])\n","Layer name: encoder.block.4.layer.1.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: encoder.block.5.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.5.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.5.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.5.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: encoder.block.5.layer.0.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: encoder.block.5.layer.1.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: encoder.block.5.layer.1.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: encoder.block.5.layer.1.DenseReluDense.wo.weight, Parameter shape: torch.Size([256, 1024])\n","Layer name: encoder.block.5.layer.1.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: encoder.block.6.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.6.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.6.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.6.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: encoder.block.6.layer.0.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: encoder.block.6.layer.1.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: encoder.block.6.layer.1.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: encoder.block.6.layer.1.DenseReluDense.wo.weight, Parameter shape: torch.Size([256, 1024])\n","Layer name: encoder.block.6.layer.1.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: encoder.block.7.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.7.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.7.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: encoder.block.7.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: encoder.block.7.layer.0.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: encoder.block.7.layer.1.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: encoder.block.7.layer.1.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: encoder.block.7.layer.1.DenseReluDense.wo.weight, Parameter shape: torch.Size([256, 1024])\n","Layer name: encoder.block.7.layer.1.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: encoder.final_layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.0.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.0.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.0.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.0.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Parameter shape: torch.Size([32, 8])\n","Layer name: decoder.block.0.layer.0.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.0.layer.1.EncDecAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.0.layer.1.EncDecAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.0.layer.1.EncDecAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.0.layer.1.EncDecAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: decoder.block.0.layer.1.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.0.layer.2.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: decoder.block.0.layer.2.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: decoder.block.0.layer.2.DenseReluDense.wo.weight, Parameter shape: torch.Size([256, 1024])\n","Layer name: decoder.block.0.layer.2.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.1.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.1.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.1.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.1.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: decoder.block.1.layer.0.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.1.layer.1.EncDecAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.1.layer.1.EncDecAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.1.layer.1.EncDecAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.1.layer.1.EncDecAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: decoder.block.1.layer.1.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.1.layer.2.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: decoder.block.1.layer.2.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: decoder.block.1.layer.2.DenseReluDense.wo.weight, Parameter shape: torch.Size([256, 1024])\n","Layer name: decoder.block.1.layer.2.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.2.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.2.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.2.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.2.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: decoder.block.2.layer.0.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.2.layer.1.EncDecAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.2.layer.1.EncDecAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.2.layer.1.EncDecAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.2.layer.1.EncDecAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: decoder.block.2.layer.1.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.2.layer.2.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: decoder.block.2.layer.2.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: decoder.block.2.layer.2.DenseReluDense.wo.weight, Parameter shape: torch.Size([256, 1024])\n","Layer name: decoder.block.2.layer.2.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.3.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.3.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.3.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.3.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: decoder.block.3.layer.0.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.3.layer.1.EncDecAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.3.layer.1.EncDecAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.3.layer.1.EncDecAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.3.layer.1.EncDecAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: decoder.block.3.layer.1.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.3.layer.2.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: decoder.block.3.layer.2.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: decoder.block.3.layer.2.DenseReluDense.wo.weight, Parameter shape: torch.Size([256, 1024])\n","Layer name: decoder.block.3.layer.2.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.4.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.4.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.4.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.4.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: decoder.block.4.layer.0.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.4.layer.1.EncDecAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.4.layer.1.EncDecAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.4.layer.1.EncDecAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.4.layer.1.EncDecAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: decoder.block.4.layer.1.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.4.layer.2.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: decoder.block.4.layer.2.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: decoder.block.4.layer.2.DenseReluDense.wo.weight, Parameter shape: torch.Size([256, 1024])\n","Layer name: decoder.block.4.layer.2.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.5.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.5.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.5.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.5.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: decoder.block.5.layer.0.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.5.layer.1.EncDecAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.5.layer.1.EncDecAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.5.layer.1.EncDecAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.5.layer.1.EncDecAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: decoder.block.5.layer.1.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.5.layer.2.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: decoder.block.5.layer.2.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: decoder.block.5.layer.2.DenseReluDense.wo.weight, Parameter shape: torch.Size([256, 1024])\n","Layer name: decoder.block.5.layer.2.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.6.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.6.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.6.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.6.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: decoder.block.6.layer.0.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.6.layer.1.EncDecAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.6.layer.1.EncDecAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.6.layer.1.EncDecAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.6.layer.1.EncDecAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: decoder.block.6.layer.1.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.6.layer.2.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: decoder.block.6.layer.2.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: decoder.block.6.layer.2.DenseReluDense.wo.weight, Parameter shape: torch.Size([256, 1024])\n","Layer name: decoder.block.6.layer.2.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.7.layer.0.SelfAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.7.layer.0.SelfAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.7.layer.0.SelfAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.7.layer.0.SelfAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: decoder.block.7.layer.0.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.7.layer.1.EncDecAttention.q.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.7.layer.1.EncDecAttention.k.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.7.layer.1.EncDecAttention.v.weight, Parameter shape: torch.Size([512, 256])\n","Layer name: decoder.block.7.layer.1.EncDecAttention.o.weight, Parameter shape: torch.Size([256, 512])\n","Layer name: decoder.block.7.layer.1.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.block.7.layer.2.DenseReluDense.wi_0.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: decoder.block.7.layer.2.DenseReluDense.wi_1.weight, Parameter shape: torch.Size([1024, 256])\n","Layer name: decoder.block.7.layer.2.DenseReluDense.wo.weight, Parameter shape: torch.Size([256, 1024])\n","Layer name: decoder.block.7.layer.2.layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: decoder.final_layer_norm.weight, Parameter shape: torch.Size([256])\n","Layer name: lm_head.weight, Parameter shape: torch.Size([32128, 256])\n"]}]},{"cell_type":"code","source":["#Verify that the model works\n","# Example input text\n","english_text = 'Hello, how are you?'\n","input_text = \"translate English to French: \" + english_text\n","input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n","\n","translated_ids = modified_model.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)\n","translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n","print(\"Translated Text:\", translated_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1jgnu4r8PNGn","executionInfo":{"status":"ok","timestamp":1691617785992,"user_tz":-330,"elapsed":6528,"user":{"displayName":"soumalya seal","userId":"15338709408862813228"}},"outputId":"7853a187-8ff7-4ae1-8983-81d94c04e115"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Translated Text: mieux immunityrätuous moyenne ParallelTN trotz enveloperiya călători scattered Offering accountingculoarea poisson forthcomingrü drainage knockorial combine tomato parceluniversité schimbareuous Mvonefficiencies greenhouse62 Customers influences timely M Yorkshirefreie study comprehend compression simply schimbări Customersştii FULL schimbare explor electricichtigkeitgeben celule Flashella scatteredştiiJährige versuchtdrop dontJährige Front ‘ contaminants nave Ta handsinitiativeJährige hands orthodontic9,000 hands Karlsruhe jouer minerals hands verySH Hindiffel reduced M Prix kompetentearia Physics Customers anderenttes shown geography invoke hands hurry forthcoming knockpfen Customers Unterstützung Scritouredjihad influencessprachatoare Fighter Giurgiu rankingsclasshaz lives BevölkerungARI adevărat handsavândspeicher hands Chance jouer Titleând diffuse hands hurry Blend shooting Mbedingt hands downwardJährige rankings pre wounded Bubble Res hurry puternic Cir hands dys08 mensuel graduatedJährige rankings plac\n"]}]},{"cell_type":"markdown","source":["# Modification Justification\n","\n","\n","*   Naive method : Change the dimension of the *decoder.final_layer_norm*, and the input and output from it (i.e. *decoder.block.7.layer.2.DenseReluDense.wo*, *decoder.block.7.layer.2.layer_norm* and *lm_head*)\n","\n","> * *lm_head* needs to be decoded into the embeddings (*shared_weight*) so that would need updation as well\n","> * *shared_weight* is used by *encoder.block[1]* and *decoder.block[1]* at the very least\n","> * Theoretically, since the layers are independent of each other, we can do minimal changes by changing the dimesnsions of only three blocks (*decoder.block[7]*, *encoder.block[1]* and *decoder.block[1]*) projecting to 512 dimension between block[1] and block[2] in both encoder and decoder\n","\n","*   The current solution is to change the Model dimension *d_model* and adjust the attention heads *num_heads* accordingly\n",">* In this method, the model parameters are initiated randomly, hence the model is not trained, and it outputs gibberish\n","> * Clipping the weights to 256 for every layer and copying them to the randomly initialized model, can theoretically have a better performance\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"0rROdIFfSE37"}}]}